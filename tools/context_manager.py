#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç ChromaDB –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤
"""

import json
import os
import hashlib
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("‚ö†Ô∏è ChromaDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install chromadb")

@dataclass
class TranslationMemory:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –ø–∞–º—è—Ç–∏"""
    source_text: str
    target_text: str
    chapter: str
    character: Optional[str] = None
    context: Optional[str] = None
    quality_score: Optional[float] = None
    timestamp: Optional[str] = None

class TranslationMemoryManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤"""
    
    def __init__(self, db_path: str = "./translation_memory"):
        self.db_path = db_path
        self.client = None
        self.collection = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω—É—é –±–∞–∑—É –∏–∑ translation_memory.json
        self.reference_data = self._load_reference_data()
        
        if CHROMADB_AVAILABLE:
            self._initialize_database()
        else:
            print("‚ùå ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞.")
            self._initialize_file_system()
    
    def _load_reference_data(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω—É—é –±–∞–∑—É –∏–∑ translation_memory.json"""
        try:
            memory_file = os.path.join(self.db_path, "translation_memory.json")
            if os.path.exists(memory_file):
                with open(memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print("‚úÖ –°–ø—Ä–∞–≤–æ—á–Ω–∞—è –±–∞–∑–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ translation_memory.json")
                return data
            else:
                print("‚ö†Ô∏è –§–∞–π–ª translation_memory.json –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø—É—Å—Ç–∞—è –±–∞–∑–∞")
                return {}
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–æ–π –±–∞–∑—ã: {e}")
            return {}
    
    def _initialize_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB"""
        try:
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(anonymized_telemetry=False)
            )
            self.collection = self.client.get_or_create_collection(
                name="translations",
                metadata={"description": "Translation memory for novel chapters"}
            )
            print("‚úÖ ChromaDB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
            self._initialize_file_system()
    
    def _initialize_file_system(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –∫–∞–∫ fallback"""
        os.makedirs(self.db_path, exist_ok=True)
        self.memory_file = os.path.join(self.db_path, "translation_memory.json")
        if not os.path.exists(self.memory_file):
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump([], f)
        print("‚úÖ –§–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def add_translation(self, memory: TranslationMemory) -> str:
        """–î–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥ –≤ –ø–∞–º—è—Ç—å"""
        if self.collection:
            return self._add_to_chromadb(memory)
        else:
            return self._add_to_file_system(memory)
    
    def _add_to_chromadb(self, memory: TranslationMemory) -> str:
        """–î–æ–±–∞–≤–∏—Ç—å –≤ ChromaDB"""
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            memory_id = hashlib.md5(
                f"{memory.source_text}_{memory.chapter}".encode()
            ).hexdigest()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "chapter": memory.chapter,
                "character": memory.character or "",
                "context": memory.context or "",
                "quality_score": memory.quality_score or 0.0,
                "timestamp": memory.timestamp or datetime.now().isoformat()
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.collection.add(
                documents=[memory.source_text],
                metadatas=[metadata],
                ids=[memory_id]
            )
            
            return memory_id
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ ChromaDB: {e}")
            return self._add_to_file_system(memory)
    
    def _add_to_file_system(self, memory: TranslationMemory) -> str:
        """–î–æ–±–∞–≤–∏—Ç—å –≤ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É"""
        try:
            # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            with open(self.memory_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
            memory_id = hashlib.md5(
                f"{memory.source_text}_{memory.chapter}".encode()
            ).hexdigest()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥
            translation_data = {
                "id": memory_id,
                "source_text": memory.source_text,
                "target_text": memory.target_text,
                "chapter": memory.chapter,
                "character": memory.character,
                "context": memory.context,
                "quality_score": memory.quality_score,
                "timestamp": memory.timestamp or datetime.now().isoformat()
            }
            
            data.append(translation_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            return memory_id
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É: {e}")
            return ""
    
    def find_similar(self, text: str, threshold: float = 0.85, max_results: int = 5) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã"""
        if self.collection:
            return self._find_similar_chromadb(text, threshold, max_results)
        else:
            return self._find_similar_file_system(text, threshold, max_results)
    
    def _find_similar_chromadb(self, text: str, threshold: float, max_results: int) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤ ChromaDB"""
        try:
            results = self.collection.query(
                query_texts=[text],
                n_results=max_results
            )
            
            similar_translations = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i] if 'distances' in results else 0.0
                    similarity = 1.0 - distance  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ —Å—Ö–æ–∂–µ—Å—Ç—å
                    
                    if similarity >= threshold:
                        similar_translations.append({
                            "source_text": doc,
                            "target_text": metadata.get("target_text", ""),
                            "chapter": metadata.get("chapter", ""),
                            "character": metadata.get("character", ""),
                            "similarity": similarity,
                            "context": metadata.get("context", "")
                        })
            
            return similar_translations
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ ChromaDB: {e}")
            return []
    
    def _find_similar_file_system(self, text: str, threshold: float, max_results: int) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)"""
        try:
            with open(self.memory_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            similar_translations = []
            for item in data:
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                source_words = set(text.lower().split())
                item_words = set(item['source_text'].lower().split())
                
                if source_words and item_words:
                    similarity = len(source_words.intersection(item_words)) / len(source_words.union(item_words))
                    
                    if similarity >= threshold:
                        similar_translations.append({
                            "source_text": item['source_text'],
                            "target_text": item['target_text'],
                            "chapter": item['chapter'],
                            "character": item.get('character', ''),
                            "similarity": similarity,
                            "context": item.get('context', '')
                        })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            similar_translations.sort(key=lambda x: x['similarity'], reverse=True)
            return similar_translations[:max_results]
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ: {e}")
            return []
    
    def get_phrase_translation(self, text: str, chapter: str = None) -> Optional[str]:
        """–ù–∞–π—Ç–∏ –≥–æ—Ç–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Ñ—Ä–∞–∑—ã –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–æ–π –±–∞–∑—ã"""
        if not self.reference_data or 'phrase_translations' not in self.reference_data:
            return None
        
        phrase_translations = self.reference_data['phrase_translations']
        
        # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for chapter_key, phrases in phrase_translations.items():
            if text in phrases:
                return phrases[text]
        
        # –ò—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for chapter_key, phrases in phrase_translations.items():
            for phrase, translation in phrases.items():
                if text.lower() in phrase.lower() or phrase.lower() in text.lower():
                    return translation
        
        return None
    
    def get_glossary_term(self, term: str) -> Optional[str]:
        """–ù–∞–π—Ç–∏ —Ç–µ—Ä–º–∏–Ω –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏"""
        if not self.reference_data or 'glossary_terms' not in self.reference_data:
            return None
        
        glossary = self.reference_data['glossary_terms']
        
        # –ò—â–µ–º –≤–æ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –≥–ª–æ—Å—Å–∞—Ä–∏—è
        for category, terms in glossary.items():
            if term in terms:
                return terms[term]
        
        return None
    
    def get_forbidden_words(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤"""
        if not self.reference_data or 'translation_errors' not in self.reference_data:
            return []
        
        errors = self.reference_data['translation_errors']
        return errors.get('forbidden_words', [])
    
    def get_character_style(self, character: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∏–ª—å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        if not self.reference_data or 'contextual_style_rules' not in self.reference_data:
            return {}
        
        style_rules = self.reference_data['contextual_style_rules']
        return style_rules.get(f"{character}_thoughts", {})
    
    def get_system_style(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∏–ª—å —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
        if not self.reference_data or 'contextual_style_rules' not in self.reference_data:
            return {}
        
        style_rules = self.reference_data['contextual_style_rules']
        return style_rules.get('system_notifications', {})
    
    def get_chapter_context(self, chapter: str, context_window: int = 3) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≥–ª–∞–≤—ã"""
        if self.collection:
            try:
                results = self.collection.query(
                    query_texts=[""],
                    where={"chapter": chapter},
                    n_results=100  # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–µ—Ä–µ–≤–æ–¥—ã –∏–∑ –≥–ª–∞–≤—ã
                )
                
                translations = []
                if results['documents'] and results['documents'][0]:
                    for i, doc in enumerate(results['documents'][0]):
                        metadata = results['metadatas'][0][i]
                        translations.append({
                            "source_text": doc,
                            "target_text": metadata.get("target_text", ""),
                            "character": metadata.get("character", ""),
                            "context": metadata.get("context", "")
                        })
                
                return translations
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã: {e}")
                return []
        else:
            # –§–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                chapter_translations = [
                    item for item in data 
                    if item.get('chapter') == chapter
                ]
                
                return chapter_translations
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã: {e}")
                return []
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–µ—Ä–µ–≤–æ–¥–∞–º"""
        if self.collection:
            try:
                count = self.collection.count()
                return {
                    "total_translations": count,
                    "database_type": "ChromaDB"
                }
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
                return {"total_translations": 0, "database_type": "ChromaDB (–æ—à–∏–±–∫–∞)"}
        else:
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                chapters = set(item.get('chapter', '') for item in data)
                characters = set(item.get('character', '') for item in data if item.get('character'))
                
                return {
                    "total_translations": len(data),
                    "chapters": len(chapters),
                    "characters": len(characters),
                    "database_type": "File System"
                }
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
                return {"total_translations": 0, "database_type": "File System (–æ—à–∏–±–∫–∞)"}

def test_translation_memory():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤"""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ò–°–¢–ï–ú–´ –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–í–û–î–û–í")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
    manager = TranslationMemoryManager()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã
    test_translations = [
        TranslationMemory(
            source_text="Jiang Chen looked at the mountain",
            target_text="–¶–∑—è–Ω –ß—ç–Ω—å –ø–æ—Å–º–æ—Ç—Ä–µ–ª –Ω–∞ –≥–æ—Ä—É",
            chapter="–ì–ª–∞–≤–∞ 1",
            character="Jiang_Chen",
            quality_score=95.0
        ),
        TranslationMemory(
            source_text="Ye Qingcheng was angry",
            target_text="–ï –¶–∏–Ω—á—ç–Ω –±—ã–ª–∞ –∑–ª–∞",
            chapter="–ì–ª–∞–≤–∞ 1", 
            character="Ye_Qingcheng",
            quality_score=90.0
        )
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã
    for translation in test_translations:
        memory_id = manager.add_translation(translation)
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ø–µ—Ä–µ–≤–æ–¥: {memory_id[:8]}...")
    
    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
    similar = manager.find_similar("Jiang Chen saw the mountain", threshold=0.5)
    print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤: {len(similar)}")
    for item in similar:
        print(f"   ‚Ä¢ {item['similarity']:.2f}: {item['target_text']}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = manager.get_statistics()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")

if __name__ == "__main__":
    test_translation_memory()
