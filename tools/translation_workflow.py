#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏ –≥–ª–∞–≤ –Ω–æ–≤–µ–ª–ª—ã
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å DeepL API –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

import os
import sys
from pathlib import Path
from datetime import datetime
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–µ–º—É –º–æ–¥—É–ª—é
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from deepl_translator import translate_chapter_with_deepl, DeepLFileTranslator
except ImportError:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å deepl_translator")
    print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install requests")
    sys.exit(1)


class ChapterTranslationManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≥–ª–∞–≤"""
    
    def __init__(self, workspace_path: str = "."):
        self.workspace_path = Path(workspace_path)
        self.journal_file = self.workspace_path / "–∂—É—Ä–Ω–∞–ª-–ø–µ—Ä–µ–≤–æ–¥–æ–≤.txt"
        self.glossary_file = self.workspace_path / "–≥–ª–æ—Å—Å–∞—Ä–∏–π.txt"
        
    def translate_chapter_deepl(self, chapter_file: str, chapter_number: int) -> dict:
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç –≥–ª–∞–≤—É —á–µ—Ä–µ–∑ DeepL API
        
        Args:
            chapter_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –≥–ª–∞–≤–æ–π
            chapter_number: –ù–æ–º–µ—Ä –≥–ª–∞–≤—ã
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        chapter_path = Path(chapter_file)
        
        if not chapter_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω: {chapter_file}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
        output_file = chapter_path.parent / f"{chapter_path.stem}-ru.txt"
        
        print(f"üöÄ –ù–∞—á–∏–Ω–∞—é –ø–µ—Ä–µ–≤–æ–¥ –ì–ª–∞–≤—ã {chapter_number} —á–µ—Ä–µ–∑ DeepL...")
        
        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ DeepL
            result = translate_chapter_with_deepl(
                english_file=str(chapter_path),
                output_file=str(output_file)
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—à–∏–º –ø—Ä–∞–≤–∏–ª–∞–º
            processed_result = self._process_deepl_result(result, chapter_number)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∂—É—Ä–Ω–∞–ª
            self._update_journal(chapter_number, "DeepL API", processed_result['quality_score'])
            
            print(f"‚úÖ –ì–ª–∞–≤–∞ {chapter_number} –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_file}")
            print(f"üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {processed_result['quality_score']}/100")
            
            return processed_result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –ì–ª–∞–≤—ã {chapter_number}: {e}")
            raise
    
    def _process_deepl_result(self, deepl_result: dict, chapter_number: int) -> dict:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç DeepL —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—à–∏–º –ø—Ä–∞–≤–∏–ª–∞–º
        
        Args:
            deepl_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç DeepL API
            chapter_number: –ù–æ–º–µ—Ä –≥–ª–∞–≤—ã
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
        """
        translated_text = deepl_result['translated']
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—à–∏–º –ø—Ä–∞–≤–∏–ª–∞–º
        processed_text = self._apply_cursorrules_processing(translated_text)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (–±–∞–∑–æ–≤–∞—è –¥–ª—è DeepL)
        quality_score = self._estimate_quality(processed_text, deepl_result['original'])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        structure_check = self._check_structure_match(
            deepl_result['original'], 
            processed_text
        )
        
        result = {
            'chapter_number': chapter_number,
            'translator': 'DeepL API',
            'translation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'original_text': deepl_result['original'],
            'raw_translation': translated_text,
            'processed_translation': processed_text,
            'quality_score': quality_score,
            'structure_match': structure_check,
            'metadata': deepl_result.get('metadata', {}),
            'needs_human_review': quality_score < 80
        }
        
        return result
    
    def _apply_cursorrules_processing(self, text) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º .cursorrules
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –æ—Ç DeepL (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º)
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –ø—Ä–∏—à–µ–ª —Å–ø–∏—Å–æ–∫
        if isinstance(text, list):
            processed = '\n'.join(text)
        else:
            processed = text
        
        # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ (—É–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)
        lines = processed.split('\n')
        normalized_lines = []
        prev_empty = False
        
        for line in lines:
            if line.strip() == '':
                if not prev_empty:
                    normalized_lines.append('')
                prev_empty = True
            else:
                normalized_lines.append(line)
                prev_empty = False
        
        processed = '\n'.join(normalized_lines)
        
        # 2. –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –æ—à–∏–±–∫–∏-–ø–µ—Ä–µ–≤–æ–¥–∞.txt
        processed = self._apply_antipatterns(processed)
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è
        processed = self._apply_glossary_terms(processed)
        
        return processed
    
    def _apply_antipatterns(self, text: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —á—Ç–µ–Ω–∏–µ–º –∏–∑ —Ñ–∞–π–ª–∞)
        antipatterns = {
            '–∫—Ä–∞–π–Ω–µ': '—á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ',
            '—Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞': '–Ω–∞–ø—Ä–∞–≤–æ –∏ –Ω–∞–ª–µ–≤–æ',
            '—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–∞': '–∏–¥–µ–∞–ª—å–Ω–∞',
            '–ù–∞ –µ–≥–æ –≤–∑–≥–ª—è–¥': '–° –µ–≥–æ —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è',
            '—Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–ª': '—Ä–∞–∑–º—ã—à–ª—è–ª',
        }
        
        processed = text
        for wrong, correct in antipatterns.items():
            processed = processed.replace(wrong, correct)
        
        return processed
    
    def _apply_glossary_terms(self, text: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –¢–µ–∫—Å—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–µ–π
        """
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –Ω–∞—à–µ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è
        glossary_terms = {
            'Slack-Off System': '–°–∏—Å—Ç–µ–º–∞ –ë–µ–∑–¥–µ–ª—å—è',
            'Slack Off System': '–°–∏—Å—Ç–µ–º–∞ –ë–µ–∑–¥–µ–ª—å—è',
            'Holy Son': '–°–≤—è—Ç–æ–π –°—ã–Ω',
            'Banished Immortal Peak': '–ü–∏–∫ –ò–∑–≥–Ω–∞–Ω–Ω–æ–≥–æ –ë–µ—Å—Å–º–µ—Ä—Ç–Ω–æ–≥–æ',
            'Primordial Holy Land': '–ò–∑–Ω–∞—á–∞–ª—å–Ω–∞—è –°–≤—è—Ç–∞—è –ó–µ–º–ª—è',
            'Nine Heavens Realm': '–¶–∞—Ä—Å—Ç–≤–æ –î–µ–≤—è—Ç–∏ –ù–µ–±–µ—Å',
            'dog licker': '–ø–æ–¥—Ö–∞–ª–∏–º',
            'slack off': '–±–µ–∑–¥–µ–ª—å–Ω–∏—á–∞—Ç—å',
            'slacking': '–±–µ–∑–¥–µ–ª—å–µ',
        }
        
        processed = text
        for en_term, ru_term in glossary_terms.items():
            processed = processed.replace(en_term, ru_term)
        
        return processed
    
    def _estimate_quality(self, translation, original) -> int:
        """
        –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞
        
        Args:
            translation: –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (—Å—Ç—Ä–æ–∫–∞)
            original: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º)
            
        Returns:
            –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç 0 –¥–æ 100
        """
        score = 70  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è DeepL
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(original, list):
            original_text = '\n'.join(original)
        else:
            original_text = original
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
        orig_lines = len([l for l in original_text.split('\n') if l.strip()])
        trans_lines = len([l for l in translation.split('\n') if l.strip()])
        
        if abs(orig_lines - trans_lines) <= 2:
            score += 10  # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–∞–≤—ã—á–µ–∫ (–¥–∏–∞–ª–æ–≥–∏)
        if '"' in original_text and '"' in translation:
            score += 5  # –î–∏–∞–ª–æ–≥–∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É (–Ω–µ –¥–æ–ª–∂–Ω–∞ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è)
        length_ratio = len(translation) / len(original_text) if len(original_text) > 0 else 1
        if 0.8 <= length_ratio <= 1.5:
            score += 10  # –î–ª–∏–Ω–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–∞—è
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
        forbidden_words = ['–∫—Ä–∞–π–Ω–µ', '—Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞', '—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–∞']
        for word in forbidden_words:
            if word in translation:
                score -= 5
        
        return min(100, max(0, score))
    
    def _check_structure_match(self, original, translation: str) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–µ—Ä–µ–≤–æ–¥–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—É
        
        Args:
            original: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º)
            translation: –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(original, list):
            original_text = '\n'.join(original)
        else:
            original_text = original
            
        orig_lines = original_text.split('\n')
        trans_lines = translation.split('\n')
        
        return {
            'original_lines': len(orig_lines),
            'translation_lines': len(trans_lines),
            'lines_match': len(orig_lines) == len(trans_lines),
            'empty_lines_original': len([l for l in orig_lines if l.strip() == '']),
            'empty_lines_translation': len([l for l in trans_lines if l.strip() == '']),
            'structure_score': 100 if len(orig_lines) == len(trans_lines) else max(0, 100 - abs(len(orig_lines) - len(trans_lines)) * 10)
        }
    
    def _update_journal(self, chapter_number: int, translator: str, quality_score: int):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –∂—É—Ä–Ω–∞–ª –ø–µ—Ä–µ–≤–æ–¥–æ–≤
        
        Args:
            chapter_number: –ù–æ–º–µ—Ä –≥–ª–∞–≤—ã
            translator: –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
            quality_score: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        try:
            date_str = datetime.now().strftime('%Y-%m-%d')
            entry = f"–ì–ª–∞–≤–∞ {chapter_number} ‚Äî –ø–µ—Ä–µ–≤—ë–ª: {translator} ‚Äî –¥–∞—Ç–∞: {date_str} ‚Äî –∫–∞—á–µ—Å—Ç–≤–æ: {quality_score}/100\n"
            
            with open(self.journal_file, 'a', encoding='utf-8') as f:
                f.write(entry)
                
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –∂—É—Ä–Ω–∞–ª: {e}")
    
    def translate_fragments_deepl(self, fragments: list) -> dict:
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ DeepL –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
        
        Args:
            fragments: –°–ø–∏—Å–æ–∫ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏
        """
        try:
            from fragment_translator import FragmentConsultant
            consultant = FragmentConsultant()
            
            print(f"ü§ñ –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å DeepL –ø–æ {len(fragments)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º...")
            full_results = consultant.consult_fragments(fragments)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            translated_fragments = [f['deepl_translation'] for f in full_results['fragments']]
            return translated_fragments
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å DeepL: {e}")
            raise


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä–µ–≤–æ–¥–∞ —á–µ—Ä–µ–∑ DeepL...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
    api_key = os.getenv('DEEPL_API_KEY')
    if not api_key:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω API –∫–ª—é—á DeepL!")
        print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
        print("   export DEEPL_API_KEY='your-key:fx'")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
    manager = ChapterTranslationManager()
    
    # –ò—â–µ–º —Ñ–∞–π–ª—ã –≥–ª–∞–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
    workspace = Path(".")
    chapter_files = list(workspace.glob("–ì–ª–∞–≤–∞ *.txt")) + list(workspace.glob("–≥–ª–∞–≤–∞ *.txt"))
    
    if not chapter_files:
        print("üìÑ –§–∞–π–ª—ã –≥–ª–∞–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≥–ª–∞–≤—É
        test_chapter = workspace / "–¢–µ—Å—Ç –ì–ª–∞–≤–∞.txt"
        with open(test_chapter, 'w', encoding='utf-8') as f:
            f.write("""Chapter Test: DeepL Integration

"Finally, she's gone!" Jiang Chen exhaled with relief.

From his perspective, apart from being brainless, Ye Qingcheng was like bad luck - better to stay as far away from her as possible.

"But how does Ye Qingcheng know about Du Guyun?" Jiang Chen stroked his chin, puzzled. According to the plot trajectory, Ye Qingcheng shouldn't know Du Guyun at this point!

"System, would it count as interfering with slacking off if I take action against Du Guyun?" Jiang Chen inquired.

"It counts!" the cold voice replied.""")
        
        chapter_files = [test_chapter]
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã
    for chapter_file in chapter_files[:1]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –¥–ª—è —Ç–µ—Å—Ç–∞
        try:
            chapter_number = 999  # –¢–µ—Å—Ç–æ–≤—ã–π –Ω–æ–º–µ—Ä
            print(f"\nüìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {chapter_file}")
            
            result = manager.translate_chapter_deepl(str(chapter_file), chapter_number)
            
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–≤–æ–¥–∞:")
            print(f"   –ö–∞—á–µ—Å—Ç–≤–æ: {result['quality_score']}/100")
            print(f"   –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ: {result['structure_match']['structure_score']}/100")
            print(f"   –¢—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {'–î–∞' if result['needs_human_review'] else '–ù–µ—Ç'}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {chapter_file}: {e}")


if __name__ == "__main__":
    main()
