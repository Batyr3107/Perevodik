#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
–í–∫–ª—é—á–∞–µ—Ç –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ embeddings, –∏–Ω–¥–µ–∫—Å—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
"""

import os
import json
import time
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import chromadb
from chromadb.config import Settings
import chromadb.utils.embedding_functions as embedding_functions
from functools import lru_cache
import numpy as np

@dataclass
class OptimizedTranslationMemory:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–≤–æ–¥–∞"""
    original_text: str
    translated_text: str
    chapter: str
    character: Optional[str] = None
    quality_score: float = 0.0
    usage_count: int = 1
    last_used: float = 0.0
    metadata: Dict[str, Any] = None

class OptimizedTranslationMemoryManager:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–µ—Ä–µ–≤–æ–¥—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, db_path: str = "translation_memory"):
        self.db_path = db_path
        self.reference_data = {}
        self.collection = None
        self.client = None
        
        # –ö—ç—à–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.glossary_cache = {}
        self.phrase_cache = {}
        self.character_style_cache = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'cache_hits': 0,
            'db_hits': 0,
            'total_queries': 0,
            'avg_query_time': 0.0
        }
        
        self._initialize()
    
    def _initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–∞–º—è—Ç–∏...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω—É—é –±–∞–∑—É
        self._load_reference_data()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        self._init_optimized_chromadb()
        
        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à–∏
        self._preload_caches()
        
        print("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ –≥–æ—Ç–æ–≤")
    
    def _load_reference_data(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω—É—é –±–∞–∑—É –∏–∑ JSON"""
        try:
            memory_file = os.path.join(self.db_path, "translation_memory.json")
            if os.path.exists(memory_file):
                with open(memory_file, 'r', encoding='utf-8') as f:
                    self.reference_data = json.load(f)
                print("‚úÖ –°–ø—Ä–∞–≤–æ—á–Ω–∞—è –±–∞–∑–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            else:
                print("‚ö†Ô∏è –§–∞–π–ª translation_memory.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
                self.reference_data = {}
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–æ–π –±–∞–∑—ã: {e}")
            self.reference_data = {}
    
    def _init_optimized_chromadb(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ChromaDB"""
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            settings = Settings(
                persist_directory=self.db_path,
                anonymized_telemetry=False,
                allow_reset=True
            )
            
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=settings
            )
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è embeddings
            embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name="all-MiniLM-L6-v2",  # –ë—ã—Å—Ç—Ä–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
                device="cpu"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            )
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            self.collection = self.client.get_or_create_collection(
                name="optimized_translations",
                embedding_function=embedding_function,
                metadata={
                    "hnsw:space": "cosine",  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
                    "hnsw:construction_ef": 200,  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    "hnsw:search_ef": 50
                }
            )
            
            print("‚úÖ ChromaDB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
            self.collection = None
    
    def _preload_caches(self):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞"""
        print("üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–µ–π...")
        
        # –ö—ç—à –≥–ª–æ—Å—Å–∞—Ä–∏—è
        if 'glossary_terms' in self.reference_data:
            for category, terms in self.reference_data['glossary_terms'].items():
                for term, translation in terms.items():
                    self.glossary_cache[term.lower()] = translation
        
        # –ö—ç—à —Ñ—Ä–∞–∑
        if 'phrase_translations' in self.reference_data:
            for chapter, phrases in self.reference_data['phrase_translations'].items():
                for phrase, translation in phrases.items():
                    self.phrase_cache[phrase.lower()] = translation
        
        # –ö—ç—à —Å—Ç–∏–ª–µ–π –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        if 'contextual_style_rules' in self.reference_data:
            for character, rules in self.reference_data['contextual_style_rules'].items():
                if 'examples' in rules:
                    self.character_style_cache[character] = rules['examples']
        
        print(f"‚úÖ –ö—ç—à–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(self.glossary_cache)} —Ç–µ—Ä–º–∏–Ω–æ–≤, {len(self.phrase_cache)} —Ñ—Ä–∞–∑")
    
    @lru_cache(maxsize=1000)
    def get_glossary_term(self, term: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ—Ä–º–∏–Ω –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)"""
        self.stats['total_queries'] += 1
        start_time = time.time()
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_term = self.glossary_cache.get(term.lower())
        if cached_term:
            self.stats['cache_hits'] += 1
            return cached_term
        
        # –ü–æ–∏—Å–∫ –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–æ–π –±–∞–∑–µ
        if 'glossary_terms' in self.reference_data:
            for category, terms in self.reference_data['glossary_terms'].items():
                if term in terms:
                    translation = terms[term]
                    self.glossary_cache[term.lower()] = translation
                    self._update_query_time(time.time() - start_time)
                    return translation
        
        self._update_query_time(time.time() - start_time)
        return term
    
    @lru_cache(maxsize=1000)
    def get_phrase_translation(self, text: str, chapter: str = None) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥ —Ñ—Ä–∞–∑—ã (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)"""
        self.stats['total_queries'] += 1
        start_time = time.time()
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_phrase = self.phrase_cache.get(text.lower())
        if cached_phrase:
            self.stats['cache_hits'] += 1
            return cached_phrase
        
        # –ü–æ–∏—Å–∫ –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–æ–π –±–∞–∑–µ
        if 'phrase_translations' in self.reference_data:
            # –ü–æ–∏—Å–∫ –ø–æ –≥–ª–∞–≤–µ
            if chapter and chapter in self.reference_data['phrase_translations']:
                phrases = self.reference_data['phrase_translations'][chapter]
                if text in phrases:
                    translation = phrases[text]
                    self.phrase_cache[text.lower()] = translation
                    self._update_query_time(time.time() - start_time)
                    return translation
            
            # –ü–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö –≥–ª–∞–≤–∞—Ö
            for chapter_phrases in self.reference_data['phrase_translations'].values():
                if text in chapter_phrases:
                    translation = chapter_phrases[text]
                    self.phrase_cache[text.lower()] = translation
                    self._update_query_time(time.time() - start_time)
                    return translation
        
        self._update_query_time(time.time() - start_time)
        return None
    
    def get_character_style(self, character: str) -> Dict[str, str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∏–ª—å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        return self.character_style_cache.get(character, {})
    
    def get_forbidden_words(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤"""
        if 'translation_errors' in self.reference_data:
            return self.reference_data['translation_errors'].get('forbidden_words', [])
        return []
    
    def search_similar_translations(self, text: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        if not self.collection:
            return []
        
        start_time = time.time()
        self.stats['total_queries'] += 1
        
        try:
            # –ü–æ–∏—Å–∫ –≤ ChromaDB
            results = self.collection.query(
                query_texts=[text],
                n_results=limit,
                include=['metadatas', 'distances', 'documents']
            )
            
            self.stats['db_hits'] += 1
            self._update_query_time(time.time() - start_time)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            similar_translations = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    similar_translations.append({
                        'text': doc,
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': results['distances'][0][i] if results['distances'] else 0.0
                    })
            
            return similar_translations
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ ChromaDB: {e}")
            return []
    
    def add_translation(self, memory: OptimizedTranslationMemory):
        """–î–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥ –≤ –ø–∞–º—è—Ç—å"""
        if not self.collection:
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            text_hash = hashlib.md5(memory.original_text.encode()).hexdigest()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'chapter': memory.chapter,
                'character': memory.character or '',
                'quality_score': memory.quality_score,
                'usage_count': memory.usage_count,
                'last_used': memory.last_used
            }
            
            if memory.metadata:
                metadata.update(memory.metadata)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ ChromaDB
            self.collection.add(
                documents=[memory.original_text],
                metadatas=[metadata],
                ids=[text_hash]
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
            self.phrase_cache[memory.original_text.lower()] = memory.translated_text
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
    
    def _update_query_time(self, query_time: float):
        """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞"""
        total_queries = self.stats['total_queries']
        current_avg = self.stats['avg_query_time']
        self.stats['avg_query_time'] = (current_avg * (total_queries - 1) + query_time) / total_queries
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        total_queries = self.stats['total_queries']
        cache_hit_rate = self.stats['cache_hits'] / max(total_queries, 1)
        
        return {
            'total_queries': total_queries,
            'cache_hits': self.stats['cache_hits'],
            'cache_hit_rate': cache_hit_rate,
            'db_hits': self.stats['db_hits'],
            'avg_query_time': self.stats['avg_query_time'],
            'glossary_cache_size': len(self.glossary_cache),
            'phrase_cache_size': len(self.phrase_cache),
            'character_style_cache_size': len(self.character_style_cache)
        }
    
    def clear_caches(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –∫—ç—à–∏"""
        self.glossary_cache.clear()
        self.phrase_cache.clear()
        self.character_style_cache.clear()
        
        # –û—á–∏—â–∞–µ–º LRU –∫—ç—à–∏
        self.get_glossary_term.cache_clear()
        self.get_phrase_translation.cache_clear()
        
        print("üßπ –í—Å–µ –∫—ç—à–∏ –æ—á–∏—â–µ–Ω—ã")
    
    def optimize_database(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        if not self.collection:
            return
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            count = self.collection.count()
            print(f"üìä –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {count} –∑–∞–ø–∏—Å–µ–π")
            
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            # –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è, —Å–∂–∞—Ç–∏–µ –∏ —Ç.–¥.
            
            print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
    manager = OptimizedTranslationMemoryManager()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞...")
    
    # –ü–æ–∏—Å–∫ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏
    term = manager.get_glossary_term("cultivation")
    print(f"–¢–µ—Ä–º–∏–Ω 'cultivation': {term}")
    
    # –ü–æ–∏—Å–∫ —Ñ—Ä–∞–∑—ã
    phrase = manager.get_phrase_translation("Hello world", "–ì–ª–∞–≤–∞ 1")
    print(f"–§—Ä–∞–∑–∞ 'Hello world': {phrase}")
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    similar = manager.search_similar_translations("Hello world", limit=3)
    print(f"–ü–æ—Ö–æ–∂–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã: {len(similar)} –Ω–∞–π–¥–µ–Ω–æ")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = manager.get_stats()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_queries']}")
    print(f"   ‚Ä¢ –ü–æ–ø–∞–¥–∞–Ω–∏—è –≤ –∫—ç—à: {stats['cache_hit_rate']:.2%}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {stats['avg_query_time']:.4f}—Å")

if __name__ == "__main__":
    main()
